{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d27fbce-819b-4584-a843-d08e6bf96064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_qa_text(question_id):\n",
    "    try:\n",
    "        return qa[qa['question_id'] == question_id].iloc[0].raw_data\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdb9b3d-55c4-4eba-93e7-cf141452e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"si630w22-hw3-train.csv\")\n",
    "dev = pd.read_csv(\"si630w22-hw3-dev.csv\")\n",
    "qa = pd.read_csv(\"si630w22-hw3-data.csv\")\n",
    "qa[\"raw_data\"] = qa[\"question_text\"] + qa[\"reply_text\"]\n",
    "# qa[\"raw_data\"] = qa[\"question_text\"]\n",
    "train[\"qa_text\"] = train[\"id\"].apply(find_qa_text)\n",
    "train[\"text\"] = train[\"annotator_id\"] * 5 + train[\"group\"] * 5 + train[\"qa_text\"] \n",
    "dev[\"qa_text\"] = dev[\"id\"].map(find_qa_text)\n",
    "dev[\"text\"] = dev[\"annotator_id\"] * 5+ dev[\"group\"] * 5 + dev[\"qa_text\"]\n",
    "final = pd.read_csv(\"si630w22-hw3-test.public.csv\")\n",
    "final[\"qa_text\"] = final[\"id\"].map(find_qa_text)\n",
    "final[\"text\"] = final[\"annotator_id\"] * 5+ final[\"group\"] * 5 + final[\"qa_text\"]\n",
    "\n",
    "# train[\"qa_text\"] = train[\"id\"].apply(find_qa_text)\n",
    "# train[\"text\"] = train[\"qa_text\"]\n",
    "# dev[\"qa_text\"] = dev[\"id\"].map(find_qa_text)\n",
    "# dev[\"text\"] = dev[\"qa_text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4239fb12-a253-4d32-bcb2-3698671de9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus1(rating):\n",
    "    return rating - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e951f4-31af-4b5c-a26d-46eacc219c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58959454-6193-4fc6-91ca-23f7418e887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = train[\"text\"].tolist(), train[\"rating\"].apply(minus1).tolist()\n",
    "test_texts, test_labels = dev[\"text\"].tolist(), dev[\"rating\"].apply(minus1).tolist()\n",
    "final_texts, final_labels = final[\"text\"].tolist(), [0 for i in range(len(final))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f669753d-4738-4ee8-8131-16f6dee7f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "max_length = 512\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "final_encodings = tokenizer(final_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "# small_train_encodings = tokenizer(train_texts[0:100], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "# small_test_encodings = tokenizer(test_texts[0:100], truncation=True, padding=\"max_length\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a322917-6972-492d-bc23-adfebdea65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelpfulnessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = HelpfulnessDataset(train_encodings, train_labels)\n",
    "test_dataset = HelpfulnessDataset(test_encodings, test_labels)\n",
    "final_dataset = HelpfulnessDataset(final_encodings, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35d6fbb-89cf-4515-8da6-56f4eba738db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = HelpfulnessDataset(small_train_encodings, train_labels[0:100])\n",
    "# small_test_dataset = HelpfulnessDataset(small_test_encodings, test_labels[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e37f893-f35f-499c-a918-84bcd78a1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = np.round_(pred.predictions)\n",
    "#   preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e0f0a9f-eff3-4f8f-aeba-88e5898195b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = torch.nn.MSELoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3bbb205-fe24-4bd4-8048-c4229e1c6d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/Multilingual-MiniLM-L12-H384\", num_labels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60c62e6-9636-4c3c-9af8-ca54093d84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=6,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.03,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=80,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c489a4-082d-4bcc-b011-29e73f0cca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangzhel/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17845\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6696\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4449' max='6696' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4449/6696 20:04 < 10:08, 3.69 it/s, Epoch 3.99/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>9.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.594100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>1.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>1.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>1.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>1.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>1.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>1.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>1.290700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>1.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>1.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.322800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n",
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "/home/fangzhel/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7f915-8005-4f34-a1c6-391d4b4a2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ccbe62-9a09-4a41-82c4-e9d16dc777f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.predict(final_dataset).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd56dd-097c-4bcd-8753-8de292d93923",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.reshape(result, 3821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5987cf3-efc6-44f7-9b71-75c9d4bb0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb486d-febe-446b-a941-ac138c6a6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"predicted\"] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a8fdb-3a31-4fba-8498-75417fc15f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rating = final.drop([\"annotator_id\",\"group\",'qa_text','text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444afef0-4326-4304-bce9-646f8894a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rating.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633a5e4-d53e-444d-9deb-c159fd4d7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mean = final_rating.groupby('id').predicted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7ded0-914e-4e87-9849-3af36ec9a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mean = rating_mean.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30c149-b359-485c-89a9-71078f9c19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mean.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1a1e3-1276-4f02-82d0-cbc34e91b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5319ea-a986-43a1-93a6-9e4f0d6f4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/Multilingual-MiniLM-L12-H384\", num_labels=1)\n",
    "# model.load_state_dict(torch.load(\"./model\"))\n",
    "# model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63112ad2-037b-4265-9c74-d0217ad6e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# qa = pd.read_csv(\"si630w22-hw3-data.csv\")\n",
    "# qa[\"raw_data\"] = qa[\"question_text\"] + qa[\"reply_text\"]\n",
    "# final = pd.read_csv(\"si630w22-hw3-test.public.csv\")\n",
    "# final[\"qa_text\"] = final[\"id\"].map(find_qa_text)\n",
    "# final[\"text\"] = final[\"annotator_id\"] * 5+ final[\"group\"] * 5 + final[\"qa_text\"]\n",
    "# final_texts = final[\"text\"].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
